function [xk, fk, gradfk_norm, k,...
    xseq, btseq, pcg_iters_seq] = modified_newton_method_preconditioning(x0, f, gradf, hessf, kmax, tolgrad, c1, btmax, n, delta, rho, h, gradf_finite_diff, hess_finite_diff, flag_h, flag_deriv)
%
% function [xk, fk, gradfk_norm, xseq, btseq, pcg_iters_seq] = modified_newton_method_preconditioning(x0, f, gradf, hessf, kmax, tolgrad, c1, btmax, n, delta, rho, h, gradf_finite_diff_rosenbrock, hess_finite_diff_rosenbrock, flag_h, flag_deriv)
% Function that performs the Newton optimization method when the hessian
% matrix is not positive definite, using bactracking strategy for the
% step-length selection.
% 
% INPUTS:
% x0 = n-dimensional colun vector;
% f = function handle that describes a function R^n->R;
% gradf = function handle that describes the gradient of f;
% Hessf = function handle that describes the Hessian of f;
% kmax = maximum number of iterations permitted;
% tolgrad = value used as stopping criterion w.r.t. the norm of the
% gradient;
% c1 = ﻿the factor of the Armijo condition that must be a scalar in (0,1);
% btmax = ﻿maximum number of steps for updating alpha during the 
% backtracking strategy.
% n = dimension of x0
% delta = parameter used to guarantee the positive definitness of Bk
% rho = ﻿fixed factor, lesser than 1, used for reducing alpha0;
% h = step-length through which we compute the expansion x = x_bar + h used
% to compute finite difference
% gradf_finite_diff = function handle that computes gradf through finite difference
% hess_finite_diff = function handle that computes hessf through finite difference
% flag_h = flag used to indicate which h to use, if flag_h = 1, need to
% adapt h at each value, if flag_h = 0, h is costant, if flag_h = 2 we
% do not use finite differences
% flag_deriv = flag used to explain if we use finite differences or correct derivatives,
% if flag_deriv = 0 we use correct derivatives, if flag_deriv = 1 finite
% differences
% 
% OUTPUTS:
% xk = the last x computed by the function with preconditioning;
% fk = the value f(xk);
% gradfk_norm = value of the norm of gradf(xk)
% k = index of the last iteration performed
% xseq = n-by-k matrix where the columns are the elements xk of the 
% sequence
% btseq = 1-by-k vector where elements are the number of backtracking
% iterations at each optimization step with xk.
% pcg_iters_seq = sequence of number of iterations needed to pcg to
% obtain the solution with preconditioning

% Function handle for the armijo condition
farmijo = @(fk, alpha, c1_gradfk_pk) ...
    fk + alpha * c1_gradfk_pk;

opts.tol = 1e-6; % Tol
opts.maxit = 100; % Number of max iterations
tol= 1e-6;
max_iter = 100;

% Inizializations
xseq = zeros(length(x0), kmax); 
btseq = zeros(1, kmax); 
pcg_iters_seq = zeros(1, kmax);


xk = x0;
fk = f(xk);

if flag_deriv == 1
    
    if flag_h == 1
        h = h*abs(xk);
    else
        h = [h; h];
    end
    gradfk  = gradf_finite_diff(xk, h);
else
    gradfk = gradf(xk);
end

k = 0;
gradfk_norm = norm(gradfk); 

while k < kmax && gradfk_norm >= tolgrad

    if flag_deriv == 1
        hessfk = hess_finite_diff(xk, h);
    else
        hessfk = hessf(xk);
    end

    lambda_min = eigs(hessfk, 1, 'smallestreal', opts);

    tauk = max([0, delta - lambda_min]);
    Ek = tauk*eye(n);
    Bk = hessfk + Ek;

    % solve with preconditioning
    Bk_sparse = sparse(Bk);
    L = ichol(Bk_sparse);
    [pk, ~, ~, pcg_iters] = pcg(Bk, -gradfk, tol, max_iter, L', L); 
    pcg_iters_seq(k+1) = pcg_iters;

    
    % Reset the value of alpha
    alpha = 1;
    flag_stag = false; % flag which indicates if there is stagnation 

    xnew = xk + alpha * pk;
    fnew = f(xnew);
    c1_gradfk_pk = c1 * gradfk' * pk;
    bt = 0;
    
    while bt < btmax && fnew > farmijo(fk, alpha, c1_gradfk_pk)
        
        alpha = rho * alpha;  
        
        if abs(alpha) <= 10^(-13) % check for stagnation
            flag_stag = true;
            break
        end

        xnew = xk + alpha* pk;
        fnew = f(xnew);
        bt = bt + 1;
    end


    if (bt == btmax && fnew > farmijo(fk, alpha, c1_gradfk_pk)) 
        break;
    end

    if bt == btmax && fnew > farmijo(fk, alpha, c1_gradfk_pk)
        warning("Couldn't find an optimal alpha for bactracking.")
        break
    end

    if flag_stag == true
        warning("Stagnation in bactracking: change btmax or rho parameters.")
        break
    end
    
    % Valueing if preconditioning is useful
    if pcg_iters == max_iter
        warning("pcg did not converge within the maximum number of iterations.");
    end
    
    % Update xk, fk, gradfk_norm

    xk = xnew;
    fk = fnew;
    if flag_deriv == 1
        gradfk  = gradf_finite_diff(xk, h);
    else
        gradfk = gradf(xk);
    end
    gradfk_norm = norm(gradfk);
    
    % Increase the step by one
    k = k + 1;
    
    % Store current xk in xseq
    xseq(:, k) = xk;

    % Store bt iterations in btseq
    btseq(k) = bt;

end

xseq = xseq(:, 1:k);
btseq = btseq(1:k);
pcg_iters_seq = pcg_iters_seq(1:k);
xseq = [x0, xseq];

